# Autoencoder Architecture

## Introduction to Autoencoders

An autoencoder is a type of neural network designed to learn efficient representations of data in an unsupervised manner. It consists of two main components:

1. **Encoder**: Compresses input data into a lower-dimensional representation
2. **Decoder**: Reconstructs the original data from the compressed representation

## CATastrophe's Autoencoder Design

### Network Architecture

```
Input Layer (2000 features)
    ↓
Encoder Hidden Layer (512 neurons, ReLU + BatchNorm + Dropout)
    ↓
Latent Space (128 neurons)
    ↓
Decoder Hidden Layer (512 neurons, ReLU + BatchNorm + Dropout)
    ↓
Output Layer (2000 features)
```

### Key Components

#### 1. Input/Output Layers
- **Dimension**: 2000 features (configurable via `MAX_FEATURES`)
- **Representation**: Vectorized code features from the feature extraction pipeline

#### 2. Encoder Network
```python
self.encoder = nn.Sequential(
    nn.Linear(input_dim, 512),
    nn.BatchNorm1d(512),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(512, 128)
)
```

#### 3. Decoder Network
```python
self.decoder = nn.Sequential(
    nn.Linear(128, 512),
    nn.BatchNorm1d(512),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(512, input_dim)
)
```

### Training Process

1. **Forward Pass**: 
   - Input features → Encoder → Latent representation → Decoder → Reconstructed features

2. **Loss Calculation**:
   - Mean Squared Error (MSE) between input and reconstructed features
   - Lower loss indicates better reconstruction

3. **Optimization**:
   - Adam optimizer with learning rate 1e-3
   - Batch size: 32
   - Early stopping with patience of 5 epochs

### Anomaly Detection

The trained autoencoder detects vulnerabilities by:

1. **Reconstruction Error Threshold**: 
   - Normal code: Low reconstruction error
   - Vulnerable code: High reconstruction error
   - Default threshold: 0.5 (configurable)

2. **Scoring Mechanism**:
   ```python
   reconstruction = model(features)
   mse_loss = F.mse_loss(reconstruction, features)
   is_vulnerable = mse_loss > threshold
   ```

## Advanced Features

### Batch Normalization
- Stabilizes training by normalizing intermediate representations
- Reduces internal covariate shift
- Enables higher learning rates

### Dropout Regularization
- 20% dropout rate prevents overfitting
- Improves generalization to unseen code patterns
- Applied during training only

### Early Stopping
- Monitors validation loss
- Stops training when no improvement for 5 epochs
- Prevents overfitting and reduces training time

## Performance Considerations

1. **Scalability**: Efficient processing of large codebases
2. **Memory Usage**: Optimized batch processing
3. **Inference Speed**: Fast vulnerability detection for real-time analysis
4. **GPU Support**: Automatic GPU utilization when available